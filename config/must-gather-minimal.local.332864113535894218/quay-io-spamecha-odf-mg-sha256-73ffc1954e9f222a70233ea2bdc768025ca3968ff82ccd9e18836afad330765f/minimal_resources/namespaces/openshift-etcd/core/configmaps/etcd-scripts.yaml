---
apiVersion: v1
data:
  cluster-backup.sh: |
    #!/usr/bin/env bash

    ### Created by cluster-etcd-operator. DO NOT edit.

    set -o errexit
    set -o pipefail
    set -o errtrace

    # example
    # cluster-backup.sh $path-to-snapshot

    if [[ $EUID -ne 0 ]]; then
      echo "This script must be run as root"
      exit 1
    fi

    function usage {
      echo 'Path to backup dir required: ./cluster-backup.sh [--force] <path-to-backup-dir>'
      exit 1
    }

    IS_DIRTY=""
    if [ "$1" == "--force" ]; then
      IS_DIRTY="__POSSIBLY_DIRTY__"
      shift
    fi

    # If the first argument is missing, or it is an existing file, then print usage and exit
    if [ -z "$1" ] || [ -f "$1" ]; then
      usage
    fi

    if [ ! -d "$1" ]; then
      mkdir -p "$1"
    fi

    function check_if_operator_is_progressing {
       local operator="$1"

       if [ ! -f "${KUBECONFIG}" ]; then
          echo "Valid kubeconfig is not found in kube-apiserver-certs. Exiting!"
          exit 1
       fi

       progressing=$(oc get co "${operator}" -o jsonpath='{.status.conditions[?(@.type=="Progressing")].status}') || true
       if [ "$progressing" == "" ]; then
          echo "Could not find the status of the $operator. Check if the API server is running. Pass the --force flag to skip checks."
          exit 1
       elif [ "$progressing" != "False" ]; then
          echo "Currently the $operator operator is progressing. A reliable backup requires that a rollout is not in progress.  Aborting!"
          exit 1
       fi
    }

    # backup latest static pod resources
    function backup_latest_kube_static_resources {
      local backup_tar_file="$1"
      local backup_resource_list=("kube-apiserver" "kube-controller-manager" "kube-scheduler" "etcd")
      local latest_resource_dirs=()

      for resource in "${backup_resource_list[@]}"; do
        if [ ! -f "/etc/kubernetes/manifests/${resource}-pod.yaml" ]; then
          echo "error finding manifests for the ${resource} pod. please check if it is running."
          exit 1
        fi

        local latest_resource
        latest_resource=$(grep -o -m 1 "/etc/kubernetes/static-pod-resources/${resource}-pod-[0-9]*" "/etc/kubernetes/manifests/${resource}-pod.yaml") || true

        if [ -z "${latest_resource}" ]; then
          echo "error finding static-pod-resources for the ${resource} pod. please check if it is running."
          exit 1
        fi
        if [ "${IS_DIRTY}" == "" ]; then
          check_if_operator_is_progressing "${resource}"
        fi

        echo "found latest ${resource}: ${latest_resource}"
        latest_resource_dirs+=("${latest_resource#${CONFIG_FILE_DIR}/}")
      done

      # tar latest resources with the path relative to CONFIG_FILE_DIR
      tar -cpzf "$backup_tar_file" -C "${CONFIG_FILE_DIR}" "${latest_resource_dirs[@]}"
      chmod 600 "$backup_tar_file"
    }

    function source_required_dependency {
      local src_path="$1"
      if [ ! -f "${src_path}" ]; then
        echo "required dependencies not found, please ensure this script is run on a node with a functional etcd static pod"
        exit 1
      fi
      # shellcheck disable=SC1090
      source "${src_path}"
    }

    BACKUP_DIR="$1"
    DATESTRING=$(date "+%F_%H%M%S")
    BACKUP_TAR_FILE=${BACKUP_DIR}/static_kuberesources_${DATESTRING}${IS_DIRTY}.tar.gz
    SNAPSHOT_FILE="${BACKUP_DIR}/snapshot_${DATESTRING}${IS_DIRTY}.db"

    trap 'rm -f ${BACKUP_TAR_FILE} ${SNAPSHOT_FILE}' ERR

    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd.env
    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd-common-tools

    # replacing the value of variables sourced form etcd.env to use the local node folders if the script is not running into the cluster-backup pod
    if [ ! -f "${ETCDCTL_CACERT}" ]; then
      echo "Certificate ${ETCDCTL_CACERT} is missing. Checking in different directory"
      export ETCDCTL_CACERT=$(echo ${ETCDCTL_CACERT} | sed -e "s|static-pod-certs|static-pod-resources/etcd-certs|")
      export ETCDCTL_CERT=$(echo ${ETCDCTL_CERT} | sed -e "s|static-pod-certs|static-pod-resources/etcd-certs|")
      export ETCDCTL_KEY=$(echo ${ETCDCTL_KEY} | sed -e "s|static-pod-certs|static-pod-resources/etcd-certs|")
      if [ ! -f "${ETCDCTL_CACERT}" ]; then
        echo "Certificate ${ETCDCTL_CACERT} is also missing in the second directory. Exiting!"
        exit 1
      else
        echo "Certificate ${ETCDCTL_CACERT} found!"
      fi
    fi

    backup_latest_kube_static_resources "${BACKUP_TAR_FILE}"

    # Download etcdctl and get the etcd snapshot
    dl_etcdctl

    # snapshot save will continue to stay in etcdctl
    ETCDCTL_ENDPOINTS="https://${NODE_NODE_ENVVAR_NAME_IP}:2379" etcdctl snapshot save "${SNAPSHOT_FILE}"

    # Check the integrity of the snapshot
    check_snapshot_status "${SNAPSHOT_FILE}"
    snapshot_failed=$?

    # If check_snapshot_status returned 1 it failed, so exit with code 1
    if [[ $snapshot_failed -eq 1 ]]; then
      echo "snapshot failed with exit code ${snapshot_failed}"
      exit 1
    fi

    echo "snapshot db and kube resources are successfully saved to ${BACKUP_DIR}"
  cluster-restore.sh: |
    #!/usr/bin/env bash

    ### Created by cluster-etcd-operator. DO NOT edit.

    set -o errexit
    set -o pipefail
    set -o errtrace

    # example
    # ./cluster-restore.sh $path-to-backup
    # There are several customization switches based on env variables:
    # ETCD_RESTORE_SKIP_MOVE_CP_STATIC_PODS - when set this script will not move the other (non-etcd) static pod yamls
    # ETCD_ETCDCTL_RESTORE - when set this script will use `etcdctl snapshot restore` instead of a restore pod yaml
    # ETCD_ETCDCTL_RESTORE_ENABLE_BUMP - when set this script will spawn the restore pod with a large enough revision bump

    if [[ $EUID -ne 0 ]]; then
      echo "This script must be run as root"
      exit 1
    fi

    function source_required_dependency {
      local src_path="$1"
      if [ ! -f "${src_path}" ]; then
        echo "required dependencies not found, please ensure this script is run on a node with a functional etcd static pod"
        exit 1
      fi
      # shellcheck disable=SC1090
      source "${src_path}"
    }

    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd.env
    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd-common-tools

    function usage() {
      echo 'Path to the directory containing backup files is required: ./cluster-restore.sh <path-to-backup>'
      echo 'The backup directory is expected to be contain two files:'
      echo '        1. etcd snapshot'
      echo '        2. A copy of the Static POD resources at the time of backup'
      exit 1
    }

    # If the argument is not passed, or if it is not a directory, print usage and exit.
    if [ "$1" == "" ] || [ ! -d "$1" ]; then
      usage
    fi

    function restore_static_pods() {
      local backup_file="$1"
      shift
      local static_pods=("$@")

      for pod_file_name in "${static_pods[@]}"; do
        backup_pod_path=$(tar -tvf "${backup_file}" "*${pod_file_name}" | awk '{ print $6 }') || true
        if [ -z "${backup_pod_path}" ]; then
          echo "${pod_file_name} does not exist in ${backup_file}"
          exit 1
        fi

        echo "starting ${pod_file_name}"
        tar -xvf "${backup_file}" --strip-components=2 -C "${MANIFEST_DIR}"/ "${backup_pod_path}"
      done
    }

    function wait_for_containers_to_stop() {
      local containers=("$@")

      for container_name in "${containers[@]}"; do
        echo "Waiting for container ${container_name} to stop"
        while [[ -n $(crictl ps --label io.kubernetes.container.name="${container_name}" -q) ]]; do
          echo -n "."
          sleep 1
        done
        echo "complete"
      done
    }

    function mv_static_pods() {
      local containers=("$@")

      # Move manifests and stop static pods
      if [ ! -d "$MANIFEST_STOPPED_DIR" ]; then
        mkdir -p "$MANIFEST_STOPPED_DIR"
      fi

      for POD_FILE_NAME in "${containers[@]}"; do
        echo "...stopping ${POD_FILE_NAME}"
        [ ! -f "${MANIFEST_DIR}/${POD_FILE_NAME}" ] && continue
        mv "${MANIFEST_DIR}/${POD_FILE_NAME}" "${MANIFEST_STOPPED_DIR}"
      done
    }

    BACKUP_DIR="$1"
    # shellcheck disable=SC2012
    BACKUP_FILE=$(ls -vd "${BACKUP_DIR}"/static_kuberesources*.tar.gz | tail -1) || true
    # shellcheck disable=SC2012
    SNAPSHOT_FILE=$(ls -vd "${BACKUP_DIR}"/snapshot*.db | tail -1) || true

    ETCD_STATIC_POD_LIST=("etcd-pod.yaml")
    AUX_STATIC_POD_LIST=("kube-apiserver-pod.yaml" "kube-controller-manager-pod.yaml" "kube-scheduler-pod.yaml")

    ETCD_STATIC_POD_CONTAINERS=("etcd" "etcdctl" "etcd-metrics" "etcd-readyz")
    AUX_STATIC_POD_CONTAINERS=("kube-controller-manager" "kube-apiserver" "kube-scheduler")

    if [ ! -f "${SNAPSHOT_FILE}" ]; then
      echo "etcd snapshot ${SNAPSHOT_FILE} does not exist"
      exit 1
    fi

    # Download etcdctl and check the snapshot status
    dl_etcdctl
    check_snapshot_status "${SNAPSHOT_FILE}"

    ETCD_CLIENT="${ETCD_ETCDCTL_BIN+etcdctl}"
    if [ -n "${ETCD_ETCDUTL_BIN}" ]; then
      ETCD_CLIENT="${ETCD_ETCDUTL_BIN}"
    fi

    # Move static pod manifests out of MANIFEST_DIR, if required
    if [ -z "${ETCD_RESTORE_SKIP_MOVE_CP_STATIC_PODS}" ]; then
      mv_static_pods "${AUX_STATIC_POD_LIST[@]}"
      wait_for_containers_to_stop "${AUX_STATIC_POD_CONTAINERS[@]}"
    fi

    # always move etcd pod and wait for all containers to exit
    mv_static_pods "${ETCD_STATIC_POD_LIST[@]}"
    wait_for_containers_to_stop "${ETCD_STATIC_POD_CONTAINERS[@]}"

    if [ ! -d "${ETCD_DATA_DIR_BACKUP}" ]; then
      mkdir -p "${ETCD_DATA_DIR_BACKUP}"
    fi

    # backup old data-dir
    if [ -d "${ETCD_DATA_DIR}/member" ]; then
      if [ -d "${ETCD_DATA_DIR_BACKUP}/member" ]; then
        echo "removing previous backup ${ETCD_DATA_DIR_BACKUP}/member"
        rm -rf "${ETCD_DATA_DIR_BACKUP}"/member
      fi
      echo "Moving etcd data-dir ${ETCD_DATA_DIR}/member to ${ETCD_DATA_DIR_BACKUP}"
      mv "${ETCD_DATA_DIR}"/member "${ETCD_DATA_DIR_BACKUP}"/
    fi

    if [ -z "${ETCD_ETCDCTL_RESTORE}" ]; then
      # Restore static pod resources
      tar -C "${CONFIG_FILE_DIR}" -xzf "${BACKUP_FILE}" static-pod-resources

      # Copy snapshot to backupdir
      cp -p "${SNAPSHOT_FILE}" "${ETCD_DATA_DIR_BACKUP}"/snapshot.db

      echo "starting restore-etcd static pod"
      # ideally this can be solved with jq and a real env var, but we don't have it available at this point
      if [ -n "${ETCD_ETCDCTL_RESTORE_ENABLE_BUMP}" ]; then
        sed "s/export ETCD_ETCDCTL_RESTORE_ENABLE_BUMP=\"false\"/export ETCD_ETCDCTL_RESTORE_ENABLE_BUMP=\"true\"/" "${RESTORE_ETCD_POD_YAML}" > "${MANIFEST_DIR}/etcd-pod.yaml"
      else
         cp -p "${RESTORE_ETCD_POD_YAML}" "${MANIFEST_DIR}/etcd-pod.yaml"
      fi

    else
      echo "removing etcd data dir..."
      rm -rf "${ETCD_DATA_DIR}"
      mkdir -p "${ETCD_DATA_DIR}"

      echo "starting snapshot restore through etcdctl..."
      if ! ${ETCD_CLIENT} snapshot restore "${SNAPSHOT_FILE}" --data-dir="${ETCD_DATA_DIR}"; then
          echo "Snapshot restore failed. Aborting!"
          exit 1
      fi

      # start the original etcd static pod again through the new snapshot
      echo "restoring old etcd pod to start etcd again"
      mv "${MANIFEST_STOPPED_DIR}/etcd-pod.yaml" "${MANIFEST_DIR}/etcd-pod.yaml"
    fi

    # start remaining static pods
    if [ -z "${ETCD_RESTORE_SKIP_MOVE_CP_STATIC_PODS}" ]; then
      restore_static_pods "${BACKUP_FILE}" "${AUX_STATIC_POD_LIST[@]}"
    fi
  etcd-common-tools: |+
    # Common environment variables
    ASSET_DIR="/home/core/assets"
    CONFIG_FILE_DIR="/etc/kubernetes"
    MANIFEST_DIR="${CONFIG_FILE_DIR}/manifests"
    ETCD_DATA_DIR="/var/lib/etcd"
    ETCD_DATA_DIR_BACKUP="/var/lib/etcd-backup"
    MANIFEST_STOPPED_DIR="${ASSET_DIR}/manifests-stopped"
    RESTORE_ETCD_POD_YAML="${CONFIG_FILE_DIR}/static-pod-resources/etcd-certs/configmaps/restore-etcd-pod/pod.yaml"
    ETCDCTL_BIN_DIR="${CONFIG_FILE_DIR}/static-pod-resources/bin"
    PATH=${PATH}:${ETCDCTL_BIN_DIR}
    export KUBECONFIG="/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig"
    export ETCD_ETCDCTL_BIN="etcdctl"

    # download etcdctl from download release image
    function dl_etcdctl {
      # Avoid caching the binary when podman exists, the etcd image is always available locally and we need a way to update etcdctl.
      # When we're running from an etcd image there's no podman and we can continue without a download.
      if ([ -n "$(command -v podman)" ]); then
         local etcdimg=${ETCD_IMAGE}
         local etcdctr=$(podman create --authfile=/var/lib/kubelet/config.json ${etcdimg})
         local etcdmnt=$(podman mount "${etcdctr}")
         [ ! -d ${ETCDCTL_BIN_DIR} ] && mkdir -p ${ETCDCTL_BIN_DIR}
         cp ${etcdmnt}/bin/etcdctl ${ETCDCTL_BIN_DIR}/
         if [ -f "${etcdmnt}/bin/etcdutl" ]; then
           cp ${etcdmnt}/bin/etcdutl ${ETCDCTL_BIN_DIR}/
           export ETCD_ETCDUTL_BIN=etcdutl
         fi

         umount "${etcdmnt}"
         podman rm "${etcdctr}"
         etcdctl version
         return
      fi

      if ([ -x "$(command -v etcdctl)" ]); then
        echo "etcdctl is already installed"
        if [ -x "$(command -v etcdutl)" ]; then
          echo "etcdutl is already installed"
          export ETCD_ETCDUTL_BIN=etcdutl
        fi

        return
      fi

      echo "Could neither pull etcdctl nor find it locally in cache. Aborting!"
      exit 1
    }

    function check_snapshot_status() {
      local snap_file="$1"

      ETCD_CLIENT="${ETCD_ETCDCTL_BIN}"
      if [ -n "${ETCD_ETCDUTL_BIN}" ]; then
        ETCD_CLIENT="${ETCD_ETCDUTL_BIN}"
      fi

      if ! ${ETCD_CLIENT} snapshot status "${snap_file}" -w json; then
        echo "Backup integrity verification failed. Backup appears corrupted. Aborting!"
        return 1
      fi
    }

  etcd.env: |
    export ALL_ETCD_ENDPOINTS="https://10.0.112.145:2379,https://10.0.47.7:2379,https://10.0.76.169:2379"
    export ETCDCTL_API="3"
    export ETCDCTL_CACERT="/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt"
    export ETCDCTL_CERT="/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-NODE_NAME.crt"
    export ETCDCTL_ENDPOINTS="https://10.0.112.145:2379,https://10.0.47.7:2379,https://10.0.76.169:2379"
    export ETCDCTL_KEY="/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-NODE_NAME.key"
    export ETCD_CIPHER_SUITES="TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256"
    export ETCD_DATA_DIR="/var/lib/etcd"
    export ETCD_ELECTION_TIMEOUT="1000"
    export ETCD_ENABLE_PPROF="true"
    export ETCD_EXPERIMENTAL_MAX_LEARNERS="3"
    export ETCD_EXPERIMENTAL_WARNING_APPLY_DURATION="200ms"
    export ETCD_EXPERIMENTAL_WATCH_PROGRESS_NOTIFY_INTERVAL="5s"
    export ETCD_HEARTBEAT_INTERVAL="100"
    export ETCD_IMAGE="quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:972eac00b1013f170963f9dec574dbbfa11b0759d7b5ee8a76e31b49e303321a"
    export ETCD_INITIAL_CLUSTER_STATE="existing"
    export ETCD_QUOTA_BACKEND_BYTES="8589934592"
    export ETCD_SOCKET_REUSE_ADDRESS="true"
    export NODE_ip_10_0_112_145_ec2_internal_ETCD_NAME="ip-10-0-112-145.ec2.internal"
    export NODE_ip_10_0_112_145_ec2_internal_ETCD_URL_HOST="10.0.112.145"
    export NODE_ip_10_0_112_145_ec2_internal_IP="10.0.112.145"
    export NODE_ip_10_0_47_7_ec2_internal_ETCD_NAME="ip-10-0-47-7.ec2.internal"
    export NODE_ip_10_0_47_7_ec2_internal_ETCD_URL_HOST="10.0.47.7"
    export NODE_ip_10_0_47_7_ec2_internal_IP="10.0.47.7"
    export NODE_ip_10_0_76_169_ec2_internal_ETCD_NAME="ip-10-0-76-169.ec2.internal"
    export NODE_ip_10_0_76_169_ec2_internal_ETCD_URL_HOST="10.0.76.169"
    export NODE_ip_10_0_76_169_ec2_internal_IP="10.0.76.169"
kind: ConfigMap
metadata:
  creationTimestamp: "2024-11-11T03:02:38Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:cluster-backup.sh: {}
        f:cluster-restore.sh: {}
        f:etcd-common-tools: {}
        f:etcd.env: {}
    manager: cluster-etcd-operator
    operation: Update
    time: "2024-11-11T03:15:49Z"
  name: etcd-scripts
  namespace: openshift-etcd
  resourceVersion: "28613"
  uid: 095fd661-a4ae-45e6-bfc6-bfa19abaf4b2
